{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD/JhuB3ZqlWvEJT0owmLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NajlaZuhir/Financial-Distress-Predictor/blob/main/V2_MLZoomCamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "iBqioYGo_Np1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import joblib\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n"
      ],
      "metadata": {
        "id": "c0_Z559m_MvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**"
      ],
      "metadata": {
        "id": "FlDW661FbIrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataframe(filename):\n",
        "    le = LabelEncoder()\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    df['Financial Distress'] = np.where(df['Financial Distress'] > -0.5, 'Distressed', 'Not Distressed')\n",
        "    df['Financial Distress'] = le.fit_transform(df['Financial Distress'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "LzcOUuGYBPTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, test_size=0.4, val_size=0.5, random_state=42):\n",
        "\n",
        "    # Step 1: Split into training and a temporary set\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    # Step 2: Split the temporary set into validation and test\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size, random_state=random_state, stratify=y_temp\n",
        "    )\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "EjjRe1BnDAjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaling_data(X_train, X_val, X_test):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled\n"
      ],
      "metadata": {
        "id": "-Vj3xrh0ECfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train_scaled, y_train):\n",
        "\n",
        "  model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    class_weight=\"balanced\",\n",
        "    solver=\"liblinear\"\n",
        "  )\n",
        "\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "FQ6e7XTbEqCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection(model, X_train, X_train_scaled, X_val_scaled, y_train, y_val):\n",
        "  feature_importance = pd.Series(\n",
        "    np.abs(model.coef_[0]),\n",
        "    index=X_train.columns\n",
        "    ).sort_values(ascending=False)\n",
        "\n",
        "  f1_scores = []\n",
        "  accuracy_scores = []\n",
        "  feature_counts = range(1, len(feature_importance) + 1)\n",
        "\n",
        "  for k in feature_counts:\n",
        "      top_features = feature_importance.index[:k]\n",
        "\n",
        "      cols_idx = [X.columns.get_loc(f) for f in top_features]\n",
        "\n",
        "      X_train_k = X_train_scaled[:, cols_idx]\n",
        "      X_val_k = X_val_scaled[:, cols_idx]\n",
        "\n",
        "      model = LogisticRegression(\n",
        "          max_iter=1000,\n",
        "          class_weight=\"balanced\",\n",
        "          solver=\"liblinear\"\n",
        "      )\n",
        "\n",
        "      model.fit(X_train_k, y_train)\n",
        "      y_pred = model.predict(X_val_k)\n",
        "\n",
        "      f1_scores.append(f1_score(y_val, y_pred, average=\"macro\"))\n",
        "      accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
        "\n",
        "  results_df = pd.DataFrame({\n",
        "    \"num_features\": feature_counts,\n",
        "    \"f1_macro\": f1_scores,\n",
        "    \"accuracy\": accuracy_scores\n",
        "  })\n",
        "\n",
        "  results_df[\"delta_f1\"] = results_df[\"f1_macro\"].diff()\n",
        "\n",
        "  threshold = 0.005\n",
        "  window = 5\n",
        "  plateau_point = None\n",
        "\n",
        "  for i in range(len(results_df) - window):\n",
        "      if results_df[\"delta_f1\"].iloc[i+1:i+window+1].abs().max() < threshold:\n",
        "          plateau_point = results_df[\"num_features\"].iloc[i]\n",
        "          break\n",
        "\n",
        "  optimal_features = feature_importance.index[:plateau_point]\n",
        "\n",
        "  return optimal_features"
      ],
      "metadata": {
        "id": "p7TuVXFhFIEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_financial_distress(new_data_df, trained_model, optimal_features):\n",
        "    X_new = new_data_df.loc[:, optimal_features]\n",
        "    y_pred = trained_model.predict(X_new)\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "2ypqmVALGbo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y, dataset_name=\"Dataset\"):\n",
        "    y_pred = model.predict(X)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    cr = classification_report(y, y_pred)\n",
        "    return acc, f1, cm, cr\n",
        "\n"
      ],
      "metadata": {
        "id": "l2JMtmcmo22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage / Testing Script**"
      ],
      "metadata": {
        "id": "MA7tPpONK1Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================\n",
        "# Step 1: Load dataset\n",
        "# =============================\n",
        "filename = \"Financial Distress.csv\"\n",
        "df = read_dataframe(filename)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.iloc[:, 3:]  # columns x1 to x83\n",
        "y = df['Financial Distress']\n",
        "\n",
        "# =============================\n",
        "# Step 2: Split data\n",
        "# =============================\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, test_size=0.4, val_size=0.5)\n",
        "\n",
        "# =============================\n",
        "# Step 3: Scale data\n",
        "# =============================\n",
        "X_train_scaled, X_val_scaled, X_test_scaled = scaling_data(X_train, X_val, X_test)\n",
        "\n",
        "# =============================\n",
        "# Step 4: Train initial model\n",
        "# =============================\n",
        "model = train_model(X_train_scaled, y_train)\n",
        "\n",
        "# =============================\n",
        "# Step 5: Feature Selection (plateau-based)\n",
        "# =============================\n",
        "optimal_features = feature_selection(\n",
        "    model,\n",
        "    X_train,\n",
        "    X_train_scaled,\n",
        "    X_val_scaled,\n",
        "    y_train,\n",
        "    y_val\n",
        ")\n",
        "\n",
        "# =============================\n",
        "# Step 6: Retrain model on optimal features\n",
        "# =============================\n",
        "# select only optimal features\n",
        "cols_idx = [X_train.columns.get_loc(f) for f in optimal_features]\n",
        "X_train_opt = X_train_scaled[:, cols_idx]\n",
        "X_val_opt = X_val_scaled[:, cols_idx]\n",
        "X_test_opt = X_test_scaled[:, cols_idx]\n",
        "\n",
        "final_model = train_model(X_train_opt, y_train)\n",
        "\n",
        "# =============================\n",
        "# Step 7: Evaluate on Validation & Test\n",
        "# =============================\n",
        "acc_val, f1_val, cm_val, cr_val = evaluate_model(final_model, X_val_opt, y_val, \"Validation Set\")\n",
        "acc_test, f1_test, cm_test, cr_test = evaluate_model(final_model, X_test_opt, y_test, \"Test Set\")\n",
        "\n",
        "# =============================\n",
        "# Step 8: Predict on new data (example)\n",
        "# =============================\n",
        "# Example: take first 5 rows of test as \"new\" data\n",
        "new_data_df = X_test[:5].copy()  # assuming X_test is a DataFrame\n",
        "predictions = predict_financial_distress(new_data_df, final_model, optimal_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLYnH_OEJ6SF",
        "outputId": "859fecd9-45fb-48ea-922d-fa835b76673c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal features:\", optimal_features)\n",
        "print(\"\\n--- Validation ---\")\n",
        "print(\"Accuracy:\", acc_val)\n",
        "print(\"F1-macro:\", f1_val)\n",
        "print(\"Confusion Matrix:\\n\", cm_val)\n",
        "print(\"Classification Report:\\n\", cr_val)\n",
        "\n",
        "print(\"\\n--- Test ---\")\n",
        "print(\"Accuracy:\", acc_test)\n",
        "print(\"F1-macro:\", f1_test)\n",
        "print(\"Confusion Matrix:\\n\", cm_test)\n",
        "print(\"Classification Report:\\n\", cr_test)\n",
        "print(\"\\nPredictions for new data:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQGxm9DyLC1x",
        "outputId": "bef5313b-70b9-436c-f9a7-0e0b85d3aa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal features: Index(['x36', 'x26', 'x7', 'x25', 'x51', 'x53', 'x5', 'x54', 'x48', 'x81',\n",
            "       'x33', 'x10', 'x13', 'x3', 'x21', 'x58', 'x30'],\n",
            "      dtype='object')\n",
            "\n",
            "--- Validation ---\n",
            "Accuracy: 0.837874659400545\n",
            "F1-macro: 0.5893754554215735\n",
            "Confusion Matrix:\n",
            " [[593 114]\n",
            " [  5  22]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.84      0.91       707\n",
            "           1       0.16      0.81      0.27        27\n",
            "\n",
            "    accuracy                           0.84       734\n",
            "   macro avg       0.58      0.83      0.59       734\n",
            "weighted avg       0.96      0.84      0.89       734\n",
            "\n",
            "\n",
            "--- Test ---\n",
            "Accuracy: 0.854421768707483\n",
            "F1-macro: 0.6097041702026292\n",
            "Confusion Matrix:\n",
            " [[605 103]\n",
            " [  4  23]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.85      0.92       708\n",
            "           1       0.18      0.85      0.30        27\n",
            "\n",
            "    accuracy                           0.85       735\n",
            "   macro avg       0.59      0.85      0.61       735\n",
            "weighted avg       0.96      0.85      0.90       735\n",
            "\n",
            "\n",
            "Predictions for new data: [0 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(model, \"financial_distress_model.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(optimal_features, \"optimal_features.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7YsDsdljcni",
        "outputId": "d98ef5db-5f98-4ef3-82cb-71b525479343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['optimal_features.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMnUOCnPjcrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}