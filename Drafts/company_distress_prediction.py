# -*- coding: utf-8 -*-
"""Company_Distress_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14rXXyWSwWaIypv2IrPDM2Dpn_3qvJOUP

<a href="https://colab.research.google.com/github/NajlaZuhir/Financial-Distress-Predictor/blob/main/V1_MLZoomCamp.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

***Classification** of **financially distressed** companies using the **CRISP-DM** framework, addressing challenges of class imbalance*

#### 1) **Business Understanding**

The **goal** of this project was to **identify financially distressed companies** — those struggling to pay employees, bills, or meet other financial obligations.

From a **business perspective, detecting distressed companies early is critical**, while misclassifying healthy companies as distressed carries a much lower cost. This makes the problem cost-sensitive, where minimizing false negatives (missed distressed firms) is the top priority.

#### 2) **Data Collection & Understanding**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('Financial Distress.csv') # Kaggle  dataset
df.head() #  Dataset Overview

print(df.shape)
print((df.isnull().sum() > 0).any()) # Check if any column has missing Values

numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df.select_dtypes(include='object').columns.tolist()

print(f"Number of numerical variables: {len(numerical_cols)}")
print(f"Number of categorical variables: {len(categorical_cols)}")

"""**Summary:**
the Goal was to know the data deeply using pandas and numpy
- Dataset size (3672, 86).
- All columns are numerical with no missing values.
- The dataset is highly imbalanced, with Class 0 being the majority class and Class 1 being the minority class.

There is many more occurencies of healthy companies than the ones under financial distress.

#### 3) **Exploratory Data Analysis**

**Feature Importance Analysis & Selection**

1) Checking for **leakage features**: to drop columns that directly encode the target
"""

# Correlation of all features with the target
corr_with_target = (
    df.corr()['Financial Distress']
    .drop('Financial Distress')
    .sort_values(ascending=False)
)

abs_corr = corr_with_target.abs().sort_values(ascending=False)

# high correlations
potential_leakage = abs_corr[abs_corr > 0.9]

len(potential_leakage)

"""2) Checking for **highly correlated features** with each other, keeping only one representative feature to:

- Reduce multicollinearity

- Improve model stability

- Simplify interpretation
"""

threshold = 0.85

X = df.drop('Financial Distress', axis=1)

corr_matrix = X.corr().abs()

upper_triangle = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

to_drop = [
    column for column in upper_triangle.columns
    if any(upper_triangle[column] > threshold)
]

df_reduced_1 = df.drop(columns=to_drop)
print(f"Number of features before filtering: {X.shape[1]}")
print(f"Number of features to drop: {len(to_drop)}")
print(f"Number of features after filtering: {df_reduced_1.shape[1]}")

"""3. Checking for **Feature-Target Correlation Filtering:** Keep only features that have a meaningful correlation with the target."""

# Compute correlation of all features with the target
cor_target = df_reduced_1.corr()['Financial Distress'].abs()

# Set a threshold, e.g., remove features with correlation < 0.05
irrelevant_features = cor_target[cor_target < 0.05].index.tolist()

# Reduced dataset
df_reduced_2 = df_reduced_1.drop(columns=irrelevant_features)
print(f"Features reduced from {df_reduced_1.shape[1]-1} to {len(irrelevant_features)}")

"""4. **Binarize** target variable, check for **inbalance distribution** & **model-based feature importance** using a Random Forest"""

# binarize

def isDistressed(x):
    """
    Returns 1 if x is less than -0.5, 0 otherwise.
    1 means Financially Distressed, 0 means Financially Healthy.
    """
    if x < -0.5:
        return 1
    elif x >= -0.5:
        return 0

df_reduced_2["Financial Distress"] = df_reduced_2["Financial Distress"].apply(isDistressed)

sns.countplot(x='Financial Distress', data=df_reduced_2)
plt.title('Target Class Distribution')
plt.show()

# Analyze Class Imbalance
print(df_reduced_2['Financial Distress'].value_counts())

print("\nClass percentage:")
print(df_reduced_2['Financial Distress'].value_counts(normalize=True) * 100)

corr = df_reduced_2.corr()
plt.figure(figsize=(12,10))
sns.heatmap(
    df_reduced_2.drop('Financial Distress', axis=1).corr(),
    cmap="coolwarm",
    center=0,
    linewidths=0.3
)
plt.title("Correlation Heatmap of Selected Features")
plt.show()

"""**Summary:**

Given the high dimensionality of the dataset (86 numerical features), traditional visualization-based EDA (e.g., histograms and pairplots) was not practical. Instead, a **feature-centric and statistically driven EDA approach** was adopted.

- Removed **leakage features** that directly encode the target variable.  
- Checked **feature correlations**:  
  - Dropped highly correlated features to avoid redundancy.  
  - Kept the one most strongly correlated with the target.  
- Removed features with **very low correlation** (correlation < 0.05) with the target.   
- Final feature set consists of **31 important features** for modeling.

#### 4) **Modelling & Evaluation**
"""

from sklearn.model_selection import train_test_split

X = df_reduced_2.drop("Financial Distress", axis=1)
y = df_reduced_2["Financial Distress"]

print("Final feature set:")
print(X.columns.tolist())
print(f"Number of features: {len(X.columns)}")


# First split: Train (70%) and Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,
    stratify=y,
    random_state=42
)

# Second split: Validation (15%) and Test (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.50,
    stratify=y_temp,
    random_state=42
)

"""**Model 1 - Baseline:** Logistic Regression"""

from sklearn.linear_model import LogisticRegression

baseline_model = LogisticRegression(
    max_iter=1000,
    class_weight="balanced",
    random_state=42
)

baseline_model.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score

y_val_pred = baseline_model.predict(X_val)
recall = recall_score(y_val, y_val_pred, pos_label=1)
f1 = f1_score(y_val, y_val_pred, pos_label=1)

print("Validation Classification Report:")
print(classification_report(y_val, y_val_pred, digits=4))

print("Confusion Matrix (Validation):")
print(confusion_matrix(y_val, y_val_pred))

print(f"\nMinority Class Recall: {recall:.4f}")
print(f"Minority Class F1-score: {f1:.4f}")

"""**Model 2:** Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    min_samples_split=5,
    min_samples_leaf=2,
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

y_val_pred_rf = rf_model.predict(X_val)
rf_recall = recall_score(y_val, y_val_pred_rf, pos_label=1)
rf_f1 = f1_score(y_val, y_val_pred_rf, pos_label=1)

print("Random Forest – Validation Report")
print(classification_report(y_val, y_val_pred_rf, digits=4))
print("Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred_rf))

print(f"\nRF Minority Recall: {rf_recall:.4f}")
print(f"RF Minority F1: {rf_f1:.4f}")

"""**Model 3:** XGB Classifier"""

from xgboost import XGBClassifier

neg, pos = y_train.value_counts()
scale_pos_weight = neg / pos

xgb_model = XGBClassifier(
    n_estimators=300,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    eval_metric="logloss",
    random_state=42
)

xgb_model.fit(X_train, y_train)

y_val_pred_xgb = xgb_model.predict(X_val)
xgb_recall = recall_score(y_val, y_val_pred_xgb, pos_label=1)
xgb_f1 = f1_score(y_val, y_val_pred_xgb, pos_label=1)

print("XGBoost – Validation Report")
print(classification_report(y_val, y_val_pred_xgb, digits=4))
print("Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred_xgb))

print(f"\nXGB Minority Recall: {xgb_recall:.4f}")
print(f"XGB Minority F1: {xgb_f1:.4f}")

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest", "XGBoost"],
    "Minority Recall": [0.8095, rf_recall, xgb_recall],
    "Minority F1": [0.2556, rf_f1, xgb_f1]
})

results

"""**Summary:**
- A Logistic Regression model with class weighting was used as an imbalance-aware baseline.
- Random Forest: Strong baseline upgrade from logistic; handles imbalance + non‑linearity.
- XGBoost: Usually #1 in recent studies for financial distress; excellent with imbalance via scale_pos_weight.

XGBoost edges out as your winner with better minority recall (0.43 vs 0.33) despite lower F1, while Random Forest sacrifices too much recall for precision.

#### 5) **Best Model:** Hyperparameter & Threshold Tuning
"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, f1_score

# Define XGBoost with basic parameters
xgb = XGBClassifier(
    objective="binary:logistic",
    eval_metric="logloss",
    use_label_encoder=False,
    scale_pos_weight=(y_train==0).sum() / (y_train==1).sum(),
    random_state=42
)

# Hyperparameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7]
}

# Use F1 for minority class as scoring
f1_scorer = make_scorer(f1_score, pos_label=1)

# RandomizedSearchCV
rand_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=50,            # can increase to 100 if time allows
    scoring=f1_scorer,
    cv=3,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

rand_search.fit(X_train, y_train)

# Best XGBoost model
best_xgb = rand_search.best_estimator_
print("Best Parameters:", rand_search.best_params_)

# Verify the model expects these features
print("Model expects these features:")
print(best_xgb.get_booster().feature_names)

# Predict probabilities on validation set
y_val_probs = best_xgb.predict_proba(X_val)[:, 1]

thresholds = np.arange(0.05, 0.51, 0.05)
results = []

for t in thresholds:
    y_pred_t = (y_val_probs >= t).astype(int)
    results.append({
        "threshold": t,
        "recall": recall_score(y_val, y_pred_t, pos_label=1),
        "f1": f1_score(y_val, y_pred_t, pos_label=1)
    })

results_df = pd.DataFrame(results)
print(results_df)

# Plot F1 vs Threshold
plt.plot(results_df['threshold'], results_df['f1'], marker='o')
plt.xlabel("Threshold")
plt.ylabel("F1-score (Minority Class)")
plt.title("Threshold Tuning for XGBoost")
plt.grid(True)
plt.show()

# Pick threshold that gives target recall >= 0.6
optimal = results_df[results_df['recall'] >= 0.6].sort_values('f1', ascending=False).iloc[0]
optimal_threshold = optimal['threshold']
print(f"Optimal Threshold: {optimal_threshold}")
print(f"Recall at this threshold: {optimal['recall']}")
print(f"F1-score at this threshold: {optimal['f1']}")

# Apply optimal threshold
y_val_pred_opt = (y_val_probs >= optimal_threshold).astype(int)
print("Validation Classification Report at Optimal Threshold:")
print(classification_report(y_val, y_val_pred_opt, digits=4))
print("Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred_opt))

"""**Summary:** Business Perspective on Model Performance

From a business standpoint, correctly identifying financially distressed companies is far more critical than mistakenly flagging healthy ones.
- A **false negative — predicting a distressed company as healthy** — can result in serious consequences, such as unpaid employees, missed bill payments, or financial collapse going unnoticed.
- In contrast, a **false positive — predicting a healthy company as distressed** — mainly leads to additional reviews or administrative checks, which are relatively low-cost.

Therefore, the model is optimized for high recall on the distressed class, even if it means accepting more false positives.

#### 6) **Deployment**
Finalizing XGBoost model with tuned hyperparameters and threshold for production-ready use.
"""

import joblib

# selected features
final_features = best_xgb.get_booster().feature_names
joblib.dump(final_features, "final_features.pkl")

# Save the XGBoost model and threshold
joblib.dump(best_xgb, "xgb_final_model.pkl")
joblib.dump(0.45, "optimal_threshold.pkl")

"""#### 7) **ML Flow**
Experiment Tracking & Model Registry
"""

!pip install mlflow

import mlflow

"""**Experiment Tracking**"""

mlflow.set_experiment("Exp-test1")
mlflow.set_tracking_uri("http://localhost:5000")

with mlflow.start_run(): # Log params, metrics, models
    mlflow.log_params(Best_Parameters)
    mlflow.log_metrics({
        'accuracy': report_dict['accuracy'],
        'recall_class_0': report_dict['0']['recall'],
        'recall_class_1': report_dict['1']['recall'],
        'f1_score_macro': report_dict['macro avg']['f1-score']
        })
    mlflow.sklearn.log_model(best_xgb, "Best XGBoost Model")

"""**Model Registry**"""

model_id = 'm-a945970151c0463c94164cc3a653599c'
model_name = 'xgboost1'

mlflow.register_model(
    model_uri=f"models:/{model_id}",
    name=model_name
)

"""**Load the model**"""

loaded_model = mlflow.sklearn.load_model(model_uri=f"models:/{model_id}")
y_pred = loaded_model.predict(X_test)
y_pred[:10]

"""**Transition from Dev enviroment to Production**"""

from mlflow.tracking import MlflowClient

client = MlflowClient()
for rm in client.search_registered_models():
    print(rm.name)

client.transition_model_version_stage(
    name="xgboost1",
    version=2,
    stage="Production",
    archive_existing_versions=True
)